{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "### The Parallel Perceptron\n\nA parallel perceptron is a structure of the single layer perceptron which involves perceptrons in finite number."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Assume that we have $n$ function and $f_{1}, f_{2}, ..., f_{n}$ are functions which are computed by these perceptrons"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The output of a perceptron which takes input $x$ as parameter is calculated as \n\\begin{equation*}\n    \\sum_{i=1}^{n} f_{i}(z) \\in \\{-n, ..., n\\}\n\\end{equation*} "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "More clearly, the result is found through $s(\\sum_{i=1}^{n} f_{i}(z))$ where $s$ is a `squashing function` which scales the output to desired range. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let $\\rho$ be the parameter of the squashing function. It denotes the resolution of the squashing function $s_{\\rho}$. We can described the function $s_{\\rho}$ as"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\\begin{equation*}\n    s_{\\rho}(p) = \n    \\begin{cases}\n    -1       & \\text{if} \\quad p < \\rho \\\\\n    p/{\\rho} & \\text{if} \\quad -\\rho \\leq p \\leq \\rho\\\\\n    1        & \\text{if} \\quad p > \\rho \\\\\n    \\end{cases}\n\\end{equation*}"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### The p-delta rule\n\nThe parallel delta rule(p-delta rule) is a simple learning algorithm which is used for the parallel perceptrons."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The approximation error of the parallel perceptron can be small as half of the quantization step size. Therefore, the algorithm can set desired accuracy $\\epsilon$ as a value. This value can be described as\n\n\\begin{equation*}\n    \\epsilon = \\frac{\\phi_{max} - \\phi_{min}}{2N}\n\\end{equation*} where $\\phi$ is a function which is equal to $s(\\sum_{i=1}^{n} f_{i}(z))$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "However, reaching to this level of the error is not guarantee. Because, the algorithm may box in local minimum error and it may not find the global minimum."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So, if the difference between desired output and actual output smaller than error, then we can say that the output of the parallel perceptron is within the desired accuracy."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this situation, the system doesn't have to modify the weights."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Consequently, we will view the situation of that the difference between the output of the parallel perceptron and the actual output is bigger than $\\epsilon$.  \n\n\\begin{equation*}\n    \\hat{y} > y + \\epsilon\n\\end{equation*}"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": " We need that the number of the weight vector which $w_{i}x \\geq 0$ to reduce the output of the parallel perceptron. The classic update rule was \n\n\\begin{equation*}\n    w_{i} \\longleftarrow w_{i} + \\xi \\Delta_{i}\n\\end{equation*} where $\\xi$ is learning rate and $\\Delta_{i}$ is the difference."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "There are some options for modifying the weights. The most advisable option is updating all weights. So, we can define $\\Delta_{i}$ as\n\n\\begin{equation*}\n    \\Delta_{i} = \n    \\begin{cases}\n        -x & \\text{if} \\quad \\hat{y} > y + \\epsilon \\quad \\text{and} \\quad w_{i}x \\geq 0\\\\\n        +x & \\text{if} \\quad \\hat{y} < y - \\epsilon \\quad \\text{and} \\quad w_{i}x < 0\\\\\n        0  &  \\text{otherwise}\n    \\end{cases}\n\\end{equation*}"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Approach of clear margin\n\nThe approach of clear margin is performed for stabilization. So, new delta rule is applied to the system with approach of clear margin. "
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\\begin{equation*}\n    \\Delta_{i} = \n    \\begin{cases}\n        -x & \\text{if} \\quad \\hat{y} > y + \\epsilon \\quad \\text{and} \\quad w_{i}x \\geq 0\\\\\n        +x & \\text{if} \\quad \\hat{y} < y - \\epsilon \\quad \\text{and} \\quad w_{i}x < 0\\\\\n        +\\frac{\\mu}{x} & \\text{if} \\quad \\hat{y} \\leq y + \\epsilon \\quad \\text{and} \\quad 0 \\leq w_{i}x < \\gamma\\\\\n        -\\frac{\\mu}{x} & \\text{if} \\quad \\hat{y} \\geq y - \\epsilon \\quad \\text{and} \\quad -\\gamma < w_{i}x < 0\\\\\n        0  &  \\text{otherwise}\n    \\end{cases}\n\\end{equation*}"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\\begin{equation*}\n    w_{i} \\longleftarrow w_{i} + \\xi \\Delta_{i}\n\\end{equation*}"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}